{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed82300",
   "metadata": {},
   "source": [
    " a good effort but the 54 gb training file csv is too much for my mac even using its gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd951fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in /Users/john/anaconda3/lib/python3.11/site-packages (2023.6.0)\n",
      "Requirement already satisfied: pandas in /Users/john/anaconda3/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: click>=8.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask) (23.2)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask) (7.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask) (3.19.0)\n",
      "Requirement already satisfied: locket in /Users/john/anaconda3/lib/python3.11/site-packages (from partd>=1.2.0->dask) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7c3c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Information:\n",
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 7 entries, id to binds\n",
      "dtypes: object(5), int64(2)None\n",
      "First few rows of the dataframe:\n",
      "   id                            buildingblock1_smiles buildingblock2_smiles  \\\n",
      "0   0  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21  C#CCOc1ccc(CN)cc1.Cl   \n",
      "1   1  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21  C#CCOc1ccc(CN)cc1.Cl   \n",
      "2   2  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21  C#CCOc1ccc(CN)cc1.Cl   \n",
      "3   3  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21  C#CCOc1ccc(CN)cc1.Cl   \n",
      "4   4  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21  C#CCOc1ccc(CN)cc1.Cl   \n",
      "\n",
      "     buildingblock3_smiles                                    molecule_smiles  \\\n",
      "0  Br.Br.NCC1CCCN1c1cccnn1  C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(N[C@@H]...   \n",
      "1  Br.Br.NCC1CCCN1c1cccnn1  C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(N[C@@H]...   \n",
      "2  Br.Br.NCC1CCCN1c1cccnn1  C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(N[C@@H]...   \n",
      "3        Br.NCc1cccc(Br)n1  C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC...   \n",
      "4        Br.NCc1cccc(Br)n1  C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC...   \n",
      "\n",
      "  protein_name  binds  \n",
      "0         BRD4      0  \n",
      "1          HSA      0  \n",
      "2          sEH      0  \n",
      "3         BRD4      0  \n",
      "4          HSA      0  \n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/john/Downloads/leash-BELKA/train.csv'\n",
    "df = dd.read_csv(file_path)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataframe Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First few rows of the dataframe:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63eaf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values - Example: fill missing values with the mean of the column\n",
    "# You can customize this as per your requirement\n",
    "#df = df.fillna(df.mode())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f88183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Filter out irrelevant or erroneous data - Example: filter out rows where a column value is below a threshold\n",
    "# You can customize this as per your requirement\n",
    "#df = df[df['some_column'] > some_threshold]\n",
    "\n",
    "# Convert to pandas dataframe for further processing if needed (optional)\n",
    "# pandas_df = df.compute()\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file (optional)\n",
    "df.to_csv('/Users/john/Downloads/leash-BELKA/cleaned_train.csv', single_file=True)\n",
    "\n",
    "print(\"Data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53402daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/john/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150aab0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'O=C(Nc1cccc(Br)c1C(=O)O)OCC1c2ccccc2-c2ccccc21'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/vlb03xdj3h15vpvyjfgvn7xc0000gp/T/ipykernel_81550/3562059247.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Standardize the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train a Random Forest Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             return_tuple = (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \"\"\"\n\u001b[1;32m    835\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 )\n\u001b[1;32m   1150\u001b[0m             ):\n\u001b[0;32m-> 1151\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \"\"\"\n\u001b[1;32m    872\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    914\u001b[0m                         )\n\u001b[1;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m                 raise ValueError(\n\u001b[1;32m    920\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                 ) from complex_warning\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m         if (\n\u001b[1;32m   2152\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'O=C(Nc1cccc(Br)c1C(=O)O)OCC1c2ccccc2-c2ccccc21'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the cleaned training dataset\n",
    "train_file_path = '/Users/john/Downloads/leash-BELKA/cleaned_train.csv'\n",
    "df_train = pd.read_csv(train_file_path)\n",
    "\n",
    "# Define features and target\n",
    "X = df_train.drop('binds', axis=1)  # Replace 'target_column' with the actual name of your target column\n",
    "y = df_train['binds']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Load the test data for which we need to make predictions\n",
    "test_file_path = '/Users/john/Downloads/leash-BELKA/test.csv'  # Replace with the actual test data file path\n",
    "df_test = pd.read_csv(test_file_path)\n",
    "\n",
    "# Assuming the test data has an identifier column and the same features as the training data\n",
    "# Replace 'id_column' with the actual identifier column name in the test data\n",
    "id_column = 'binds'  # Replace with the actual name of the identifier column\n",
    "X_submission = df_test.drop(id_column, axis=1)\n",
    "\n",
    "# Standardize the test features\n",
    "X_submission = scaler.transform(X_submission)\n",
    "\n",
    "# Make predictions on the test data\n",
    "submission_predictions = model.predict(X_submission)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    id_column: df_test[id_column],\n",
    "    'predictions': submission_predictions\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_file_path = '/User/john/Downloads/leash-BELKA/submission.csv'\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"Submission file created at: {submission_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6be7205",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 25.8 PiB for an array with shape (295246830, 98415610) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Convert categorical features to numerical using one-hot encoding\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(X)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[1;32m     19\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:214\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m     with_dummies \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39mdtypes_to_encode)]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, pre, sep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_to_encode\u001b[38;5;241m.\u001b[39mitems(), prefix, prefix_sep):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# col is (column_name, column), use just column data here\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     dummy \u001b[38;5;241m=\u001b[39m _get_dummies_1d(\n\u001b[1;32m    215\u001b[0m         col[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    216\u001b[0m         prefix\u001b[38;5;241m=\u001b[39mpre,\n\u001b[1;32m    217\u001b[0m         prefix_sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m    218\u001b[0m         dummy_na\u001b[38;5;241m=\u001b[39mdummy_na,\n\u001b[1;32m    219\u001b[0m         sparse\u001b[38;5;241m=\u001b[39msparse,\n\u001b[1;32m    220\u001b[0m         drop_first\u001b[38;5;241m=\u001b[39mdrop_first,\n\u001b[1;32m    221\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    222\u001b[0m     )\n\u001b[1;32m    223\u001b[0m     with_dummies\u001b[38;5;241m.\u001b[39mappend(dummy)\n\u001b[1;32m    224\u001b[0m result \u001b[38;5;241m=\u001b[39m concat(with_dummies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:353\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     dummy_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_\n\u001b[0;32m--> 353\u001b[0m dummy_mat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdummy_dtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    354\u001b[0m dummy_mat[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(codes)), codes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dummy_na:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# reset NaN GH4446\u001b[39;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 25.8 PiB for an array with shape (295246830, 98415610) and data type bool"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the cleaned training dataset\n",
    "train_file_path = '/Users/john/Downloads/leash-BELKA/cleaned_train.csv'\n",
    "df_train = pd.read_csv(train_file_path)\n",
    "\n",
    "# Define features and target\n",
    "X = df_train.drop('binds', axis=1)\n",
    "y = df_train['binds']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Load the test data for which we need to make predictions\n",
    "test_file_path = '/Users/john/Downloads/leash-BELKA/test.csv'  # Replace with the actual test data file path\n",
    "df_test = pd.read_csv(test_file_path)\n",
    "\n",
    "# Assuming the test data has an identifier column and the same features as the training data\n",
    "id_column = 'binds'  # Replace with the actual name of the identifier column\n",
    "X_submission = df_test.drop(id_column, axis=1)\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X_submission = pd.get_dummies(X_submission)\n",
    "\n",
    "# Ensure the same columns as in training\n",
    "X_submission = X_submission.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Standardize the test features\n",
    "X_submission = scaler.transform(X_submission)\n",
    "\n",
    "# Make predictions on the test data\n",
    "submission_predictions = model.predict(X_submission)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    id_column: df_test[id_column],\n",
    "    'predictions': submission_predictions\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_file_path = '/Users/john/Downloads/leash-BELKA/submission.csv'\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"Submission file created at: {submission_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8226161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask-ml\n",
      "  Downloading dask_ml-2024.4.4-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting dask-glm>=0.2.0 (from dask-ml)\n",
      "  Downloading dask_glm-0.3.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: dask>=2.4.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2023.6.0)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (2023.6.0)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (0.6.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (0.57.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (23.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (1.11.1)\n",
      "Requirement already satisfied: click>=8.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (2023.10.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (7.0.0)\n",
      "Collecting sparse>=0.7.0 (from dask-glm>=0.2.0->dask-ml)\n",
      "  Downloading sparse-0.15.4-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (3.1.4)\n",
      "Requirement already satisfied: locket>=1.0.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (1.0.3)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (5.9.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (6.3.2)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (2.2.1)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (2.2.0)\n",
      "Requirement already satisfied: six in /Users/john/anaconda3/lib/python3.11/site-packages (from multipledispatch>=0.4.9->dask-ml) (1.16.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/john/anaconda3/lib/python3.11/site-packages (from numba>=0.51.0->dask-ml) (0.40.0)\n",
      "Collecting numpy>=1.20.0 (from dask-ml)\n",
      "  Downloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->dask-ml) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->dask-ml) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->dask-ml) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.2.0->dask-ml) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.2.0->dask-ml) (2.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask>=2.4.0->dask[array,dataframe]>=2.4.0->dask-ml) (3.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.1.5)\n",
      "Requirement already satisfied: heapdict in /Users/john/anaconda3/lib/python3.11/site-packages (from zict>=2.2.0->distributed>=2.4.0->dask-ml) (1.0.1)\n",
      "Downloading dask_ml-2024.4.4-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.8/149.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dask_glm-0.3.2-py2.py3-none-any.whl (13 kB)\n",
      "Downloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sparse-0.15.4-py2.py3-none-any.whl (237 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.3/237.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, sparse, dask-glm, dask-ml\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "transformers 4.36.2 requires tokenizers<0.19,>=0.14, but you have tokenizers 0.19.1 which is incompatible.\n",
      "embedchain 0.1.103 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.2.1 which is incompatible.\n",
      "embedchain 0.1.103 requires langchain-openai<0.0.6,>=0.0.5, but you have langchain-openai 0.1.8rc1 which is incompatible.\n",
      "embedchain 0.1.103 requires tiktoken<0.6.0,>=0.5.2, but you have tiktoken 0.7.0 which is incompatible.\n",
      "matplotlib 3.7.2 requires pyparsing<3.1,>=2.3.1, but you have pyparsing 3.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dask-glm-0.3.2 dask-ml-2024.4.4 numpy-1.24.4 sparse-0.15.4\n"
     ]
    }
   ],
   "source": [
    "!/Users/john/anaconda3/bin/python -m pip install dask-ml --trusted-host pypi.org --trusted-host files.pythonhosted.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee731678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[complete] in /Users/john/anaconda3/lib/python3.11/site-packages (2023.6.0)\n",
      "Requirement already satisfied: click>=8.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (23.2)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (7.0.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (14.0.2)\n",
      "Requirement already satisfied: lz4>=4.3.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (4.3.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.19.0)\n",
      "Requirement already satisfied: locket in /Users/john/anaconda3/lib/python3.11/site-packages (from partd>=1.2.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/john/anaconda3/lib/python3.11/site-packages (from pyarrow>=7.0->dask[complete]) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (2.2.0)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (3.2.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (3.1.4)\n",
      "Requirement already satisfied: distributed==2023.6.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (2023.6.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed==2023.6.0->dask[complete]) (1.0.3)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed==2023.6.0->dask[complete]) (5.9.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed==2023.6.0->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed==2023.6.0->dask[complete]) (1.7.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed==2023.6.0->dask[complete]) (6.3.2)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed==2023.6.0->dask[complete]) (2.2.1)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed==2023.6.0->dask[complete]) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1 in /Users/john/anaconda3/lib/python3.11/site-packages (from bokeh>=2.4.2->dask[complete]) (1.0.5)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from bokeh>=2.4.2->dask[complete]) (10.3.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from bokeh>=2.4.2->dask[complete]) (2022.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from jinja2>=2.10.3->dask[complete]) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas>=1.3->dask[complete]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas>=1.3->dask[complete]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas>=1.3->dask[complete]) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->dask[complete]) (1.16.0)\n",
      "Requirement already satisfied: heapdict in /Users/john/anaconda3/lib/python3.11/site-packages (from zict>=2.2.0->distributed==2023.6.0->dask[complete]) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "#pip install dask[complete]\n",
    "#run pip commands this way here:\n",
    "!/Users/john/anaconda3/bin/python -m pip install dask[complete] --trusted-host pypi.org --trusted-host files.pythonhosted.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f2ccab6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RandomForestClassifier' from 'dask_ml.ensemble' (/Users/john/anaconda3/lib/python3.11/site-packages/dask_ml/ensemble/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorizer, DummyEncoder, StandardScaler\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix, classification_report\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RandomForestClassifier' from 'dask_ml.ensemble' (/Users/john/anaconda3/lib/python3.11/site-packages/dask_ml/ensemble/__init__.py)"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder, StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.ensemble import RandomForestClassifier\n",
    "from dask_ml.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned training dataset\n",
    "train_file_path = '/Users/john/Downloads/leash-BELKA/cleaned_train.csv'\n",
    "df_train = dd.read_csv(train_file_path)\n",
    "\n",
    "# Define features and target\n",
    "X = df_train.drop('binds', axis=1)\n",
    "y = df_train['binds']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "categorizer = Categorizer()\n",
    "X = categorizer.fit_transform(X)\n",
    "encoder = DummyEncoder()\n",
    "X = encoder.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix.compute())\n",
    "print(\"Classification Report:\")\n",
    "print(class_report.compute())\n",
    "\n",
    "# Load the test data for which we need to make predictions\n",
    "test_file_path = '/Users/john/Downloads/leash-BELKA/test.csv'  # Replace with the actual test data file path\n",
    "df_test = dd.read_csv(test_file_path)\n",
    "\n",
    "# Assuming the test data has an identifier column and the same features as the training data\n",
    "id_column = 'binds'  # Replace with the actual name of the identifier column\n",
    "X_submission = df_test.drop(id_column, axis=1)\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X_submission = categorizer.transform(X_submission)\n",
    "X_submission = encoder.transform(X_submission)\n",
    "\n",
    "# Ensure the same columns as in training\n",
    "X_submission = X_submission.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Standardize the test features\n",
    "X_submission = scaler.transform(X_submission)\n",
    "\n",
    "# Make predictions on the test data\n",
    "submission_predictions = model.predict(X_submission)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = dd.concat([df_test[id_column], submission_predictions], axis=1)\n",
    "submission_df.columns = [id_column, 'predictions']\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_file_path = '/Users/john/Downloads/leash-BELKA/submission.csv'\n",
    "submission_df.to_csv(submission_file_path, single_file=True, index=False)\n",
    "\n",
    "print(f\"Submission file created at: {submission_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d83a18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'confusion_matrix' from 'dask_ml.metrics' (/Users/john/anaconda3/lib/python3.11/site-packages/dask_ml/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix, classification_report\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the cleaned training dataset\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'confusion_matrix' from 'dask_ml.metrics' (/Users/john/anaconda3/lib/python3.11/site-packages/dask_ml/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder, StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask_ml.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned training dataset\n",
    "train_file_path = '/Users/john/Downloads/leash-BELKA/cleaned_train.csv'\n",
    "df_train = dd.read_csv(train_file_path)\n",
    "\n",
    "# Define features and target\n",
    "X = df_train.drop('binds', axis=1)\n",
    "y = df_train['binds']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "categorizer = Categorizer()\n",
    "X = categorizer.fit_transform(X)\n",
    "encoder = DummyEncoder()\n",
    "X = encoder.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays for sklearn\n",
    "X_train = X_train.compute()\n",
    "X_test = X_test.compute()\n",
    "y_train = y_train.compute()\n",
    "y_test = y_test.compute()\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Load the test data for which we need to make predictions\n",
    "test_file_path = '/Users/john/Downloads/leash-BELKA/test.csv'  # Replace with the actual test data file path\n",
    "df_test = dd.read_csv(test_file_path)\n",
    "\n",
    "# Assuming the test data has an identifier column and the same features as the training data\n",
    "id_column = 'binds'  # Replace with the actual name of the identifier column\n",
    "X_submission = df_test.drop(id_column, axis=1)\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X_submission = categorizer.transform(X_submission)\n",
    "X_submission = encoder.transform(X_submission)\n",
    "\n",
    "# Ensure the same columns as in training\n",
    "X_submission = X_submission.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Standardize the test features\n",
    "X_submission = scaler.transform(X_submission)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays for sklearn\n",
    "X_submission = X_submission.compute()\n",
    "\n",
    "# Make predictions on the test data\n",
    "submission_predictions = model.predict(X_submission)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    id_column: df_test[id_column].compute(),\n",
    "    'predictions': submission_predictions\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_file_path = '/Users/john/Downloads/leash-BELKA/submission.csv'\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"Submission file created at: {submission_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ce732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder, StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned training dataset\n",
    "train_file_path = '/Users/john/Downloads/leash-BELKA/cleaned_train.csv'\n",
    "df_train = dd.read_csv(train_file_path)\n",
    "\n",
    "# Define features and target\n",
    "X = df_train.drop('binds', axis=1)\n",
    "y = df_train['binds']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "categorizer = Categorizer()\n",
    "X = categorizer.fit_transform(X)\n",
    "encoder = DummyEncoder()\n",
    "X = encoder.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays for sklearn\n",
    "X_train = X_train.compute()\n",
    "X_test = X_test.compute()\n",
    "y_train = y_train.compute()\n",
    "y_test = y_test.compute()\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Load the test data for which we need to make predictions\n",
    "test_file_path = '/Users/john/Downloads/leash-BELKA/test.csv'  # Replace with the actual test data file path\n",
    "df_test = dd.read_csv(test_file_path)\n",
    "\n",
    "# Assuming the test data has an identifier column and the same features as the training data\n",
    "id_column = 'binds'  # Replace with the actual name of the identifier column\n",
    "X_submission = df_test.drop(id_column, axis=1)\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X_submission = categorizer.transform(X_submission)\n",
    "X_submission = encoder.transform(X_submission)\n",
    "\n",
    "# Ensure the same columns as in training\n",
    "X_submission = X_submission.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Standardize the test features\n",
    "X_submission = scaler.transform(X_submission)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays for sklearn\n",
    "X_submission = X_submission.compute()\n",
    "\n",
    "# Make predictions on the test data\n",
    "submission_predictions = model.predict(X_submission)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    id_column: df_test[id_column].compute(),\n",
    "    'predictions': submission_predictions\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_file_path = '/Users/john/Downloads/leash-BELKA/submission.csv'\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"Submission file created at: {submission_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7afd801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask-ml in /Users/john/anaconda3/lib/python3.11/site-packages (2024.4.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/john/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: dask[complete] in /Users/john/anaconda3/lib/python3.11/site-packages (2023.6.0)\n",
      "Requirement already satisfied: click>=8.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (23.2)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (7.0.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (14.0.2)\n",
      "Requirement already satisfied: lz4>=4.3.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (4.3.2)\n",
      "Requirement already satisfied: dask-glm>=0.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (0.3.2)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (2023.6.0)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (0.6.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (0.57.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (1.24.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (2.2.0)\n",
      "Requirement already satisfied: scipy in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-ml) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: sparse>=0.7.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask-glm>=0.2.0->dask-ml) (0.15.4)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (3.1.4)\n",
      "Requirement already satisfied: locket>=1.0.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (1.0.3)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (5.9.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (6.3.2)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (2.2.1)\n",
      "Requirement already satisfied: zict>=2.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from distributed>=2.4.0->dask-ml) (2.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.19.0)\n",
      "Requirement already satisfied: six in /Users/john/anaconda3/lib/python3.11/site-packages (from multipledispatch>=0.4.9->dask-ml) (1.16.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/john/anaconda3/lib/python3.11/site-packages (from numba>=0.51.0->dask-ml) (0.40.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->dask-ml) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->dask-ml) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/john/anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->dask-ml) (2024.1)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in /Users/john/anaconda3/lib/python3.11/site-packages (from dask[complete]) (3.2.1)\n",
      "Requirement already satisfied: contourpy>=1 in /Users/john/anaconda3/lib/python3.11/site-packages (from bokeh>=2.4.2->dask[complete]) (1.0.5)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from bokeh>=2.4.2->dask[complete]) (10.3.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from bokeh>=2.4.2->dask[complete]) (2022.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.1.5)\n",
      "Requirement already satisfied: heapdict in /Users/john/anaconda3/lib/python3.11/site-packages (from zict>=2.2.0->distributed>=2.4.0->dask-ml) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!/Users/john/anaconda3/bin/python -m pip install dask[complete] dask-ml scikit-learn --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69adcb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 57.1M  100 57.1M    0     0  26.3M      0  0:00:02  0:00:02 --:--:-- 49.8M\n"
     ]
    }
   ],
   "source": [
    "!curl -LO https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a69e9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miniforge3-MacOSX-arm64.sh: illegal option -- y\r\n",
      "ERROR: did not recognize option '?', please try -h\r\n"
     ]
    }
   ],
   "source": [
    "!bash Miniforge3-MacOSX-arm64.sh -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1628342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3/envs/tensorflow-metal\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.9\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    libffi-3.4.4               |       hca03da5_1         120 KB\n",
      "    pip-24.0                   |   py39hca03da5_0         2.6 MB\n",
      "    python-3.9.19              |       hb885b13_1        11.7 MB\n",
      "    setuptools-69.5.1          |   py39hca03da5_0        1004 KB\n",
      "    sqlite-3.45.3              |       h80987f9_0         1.2 MB\n",
      "    tk-8.6.14                  |       h6ba3021_0         3.3 MB\n",
      "    tzdata-2024a               |       h04d1e81_0         116 KB\n",
      "    wheel-0.43.0               |   py39hca03da5_0         107 KB\n",
      "    xz-5.4.6                   |       h80987f9_1         371 KB\n",
      "    zlib-1.2.13                |       h18a0788_1          91 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        20.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  ca-certificates    pkgs/main/osx-arm64::ca-certificates-2024.3.11-hca03da5_0 \n",
      "  libcxx             pkgs/main/osx-arm64::libcxx-14.0.6-h848a8c0_0 \n",
      "  libffi             pkgs/main/osx-arm64::libffi-3.4.4-hca03da5_1 \n",
      "  ncurses            pkgs/main/osx-arm64::ncurses-6.4-h313beb8_0 \n",
      "  openssl            pkgs/main/osx-arm64::openssl-3.0.13-h1a28f6b_2 \n",
      "  pip                pkgs/main/osx-arm64::pip-24.0-py39hca03da5_0 \n",
      "  python             pkgs/main/osx-arm64::python-3.9.19-hb885b13_1 \n",
      "  readline           pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 \n",
      "  setuptools         pkgs/main/osx-arm64::setuptools-69.5.1-py39hca03da5_0 \n",
      "  sqlite             pkgs/main/osx-arm64::sqlite-3.45.3-h80987f9_0 \n",
      "  tk                 pkgs/main/osx-arm64::tk-8.6.14-h6ba3021_0 \n",
      "  tzdata             pkgs/main/noarch::tzdata-2024a-h04d1e81_0 \n",
      "  wheel              pkgs/main/osx-arm64::wheel-0.43.0-py39hca03da5_0 \n",
      "  xz                 pkgs/main/osx-arm64::xz-5.4.6-h80987f9_1 \n",
      "  zlib               pkgs/main/osx-arm64::zlib-1.2.13-h18a0788_1 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "python-3.9.19        | 11.7 MB   |                                       |   0% \n",
      "tk-8.6.14            | 3.3 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "pip-24.0             | 2.6 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "sqlite-3.45.3        | 1.2 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "setuptools-69.5.1    | 1004 KB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xz-5.4.6             | 371 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libffi-3.4.4         | 120 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tzdata-2024a         | 116 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "wheel-0.43.0         | 107 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-1.2.13          | 91 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "setuptools-69.5.1    | 1004 KB   | 5                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "python-3.9.19        | 11.7 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "pip-24.0             | 2.6 MB    | 2                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "sqlite-3.45.3        | 1.2 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "setuptools-69.5.1    | 1004 KB   | #################6                    |  48% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "python-3.9.19        | 11.7 MB   | #1                                    |   3% \u001b[A\n",
      "\n",
      "\n",
      "sqlite-3.45.3        | 1.2 MB    | ###############2                      |  41% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pip-24.0             | 2.6 MB    | #####9                                |  16% \u001b[A\u001b[A\n",
      "tk-8.6.14            | 3.3 MB    | ########7                             |  24% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-3.9.19        | 11.7 MB   | ###8                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pip-24.0             | 2.6 MB    | #######################4              |  63% \u001b[A\u001b[A\n",
      "tk-8.6.14            | 3.3 MB    | ####################################5 |  99% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tzdata-2024a         | 116 KB    | #####                                 |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libffi-3.4.4         | 120 KB    | ####9                                 |  13% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "sqlite-3.45.3        | 1.2 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "python-3.9.19        | 11.7 MB   | ###############                       |  41% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-3.9.19        | 11.7 MB   | #################################4    |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-1.2.13          | 91 KB     | ######5                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xz-5.4.6             | 371 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xz-5.4.6             | 371 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "setuptools-69.5.1    | 1004 KB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "setuptools-69.5.1    | 1004 KB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "tk-8.6.14            | 3.3 MB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tzdata-2024a         | 116 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tzdata-2024a         | 116 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libffi-3.4.4         | 120 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libffi-3.4.4         | 120 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "wheel-0.43.0         | 107 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "wheel-0.43.0         | 107 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-1.2.13          | 91 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-1.2.13          | 91 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "pip-24.0             | 2.6 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate tensorflow-metal\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...\n",
      "conda: error: unrecognized arguments: -y\n"
     ]
    }
   ],
   "source": [
    "!conda create -n tensorflow-metal python=3.9 -y\n",
    "!conda activate tensorflow-metal -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2255f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - apple\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: / \u001b[33m\u001b[1mwarning  libmamba\u001b[m Added empty dependency for problem type SOLVER_RULE_UPDATE\n",
      "failed\n",
      "\n",
      "LibMambaUnsatisfiableError: Encountered problems while solving:\n",
      "  - nothing provides numpy >=1.23.2,<1.23.3 needed by tensorflow-deps-2.10.0-0\n",
      "\n",
      "Could not solve for environment specs\n",
      "The following packages are incompatible\n",
      "├─ \u001b[32mpin-1\u001b[0m is installable and it requires\n",
      "│  └─ \u001b[32mpython 3.11.* \u001b[0m, which can be installed;\n",
      "└─ \u001b[31mtensorflow-deps\u001b[0m is not installable because there are no viable options\n",
      "   ├─ \u001b[31mtensorflow-deps 2.10.0\u001b[0m would require\n",
      "   │  └─ \u001b[31mnumpy >=1.23.2,<1.23.3 \u001b[0m, which does not exist (perhaps a missing channel);\n",
      "   ├─ \u001b[31mtensorflow-deps [2.5.0|2.6.0|2.7.0]\u001b[0m would require\n",
      "   │  └─ \u001b[31mabsl-py >=0.10,<0.11 \u001b[0m, which does not exist (perhaps a missing channel);\n",
      "   └─ \u001b[31mtensorflow-deps [2.8.0|2.9.0]\u001b[0m would require\n",
      "      ├─ \u001b[31mh5py >=3.6.0,<3.7 \u001b[0m but there are no viable options\n",
      "      │  ├─ \u001b[31mh5py 3.6.0\u001b[0m would require\n",
      "      │  │  └─ \u001b[31mpython >=3.10,<3.11.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n",
      "      │  ├─ \u001b[31mh5py 3.6.0\u001b[0m would require\n",
      "      │  │  └─ \u001b[31mpython >=3.8,<3.9.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n",
      "      │  └─ \u001b[31mh5py 3.6.0\u001b[0m would require\n",
      "      │     └─ \u001b[31mpython >=3.9,<3.10.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n",
      "      └─ \u001b[31mnumpy >=1.21.2,<1.22 \u001b[0m but there are no viable options\n",
      "         ├─ \u001b[31mnumpy [1.21.2|1.21.5|1.21.6]\u001b[0m would require\n",
      "         │  └─ \u001b[31mpython >=3.10,<3.11.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n",
      "         ├─ \u001b[31mnumpy [1.21.2|1.21.5|1.21.6]\u001b[0m would require\n",
      "         │  └─ \u001b[31mpython >=3.8,<3.9.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n",
      "         └─ \u001b[31mnumpy [1.21.2|1.21.5|1.21.6]\u001b[0m would require\n",
      "            └─ \u001b[31mpython >=3.9,<3.10.0a0 \u001b[0m, which conflicts with any installable versions previously reported.\n",
      "\n",
      "Collecting tensorflow-macos\n",
      "  Using cached tensorflow_macos-2.16.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow==2.16.1 (from tensorflow-macos)\n",
      "  Using cached tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (24.3.25)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (70.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (4.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (1.64.0)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow==2.16.1->tensorflow-macos) (1.24.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.16.1->tensorflow-macos) (0.38.4)\n",
      "Requirement already satisfied: rich in /Users/john/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.1->tensorflow-macos) (13.7.1)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached optree-0.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (45 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/john/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->tensorflow-macos) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/john/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->tensorflow-macos) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->tensorflow-macos) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/john/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.1->tensorflow-macos) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1->tensorflow-macos) (3.6)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow==2.16.1->tensorflow-macos)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1->tensorflow-macos) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1->tensorflow-macos) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.1->tensorflow-macos) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.1->tensorflow-macos) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/john/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1->tensorflow-macos) (0.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached tensorflow_macos-2.16.1-cp311-cp311-macosx_12_0_arm64.whl (2.2 kB)\n",
      "Using cached tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl (227.0 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.0-cp311-cp311-macosx_12_0_arm64.whl (3.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.11.0-cp311-cp311-macosx_11_0_arm64.whl (274 kB)\n",
      "Installing collected packages: namex, libclang, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, h5py, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow, tensorflow-macos\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 gast-0.5.4 google-pasta-0.2.0 h5py-3.11.0 keras-3.3.3 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 tensorflow-macos-2.16.1\n",
      "Collecting tensorflow-metal\n",
      "  Using cached tensorflow_metal-1.1.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow-metal) (0.38.4)\n",
      "Requirement already satisfied: six>=1.15.0 in /Users/john/anaconda3/lib/python3.11/site-packages (from tensorflow-metal) (1.16.0)\n",
      "Using cached tensorflow_metal-1.1.0-cp311-cp311-macosx_12_0_arm64.whl (1.4 MB)\n",
      "Installing collected packages: tensorflow-metal\n",
      "Successfully installed tensorflow-metal-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!conda install -c apple tensorflow-deps\n",
    "#!/Users/john/anaconda3/bin/python -m pip install dask[complete] --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "\n",
    "\n",
    "#!pip install tensorflow-macos\n",
    "!/Users/john/anaconda3/bin/python -m pip install tensorflow-macos --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
    "#!pip install tensorflow-metal\n",
    "!/Users/john/anaconda3/bin/python -m pip install tensorflow-metal --trusted-host pypi.org --trusted-host files.pythonhosted.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf35157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43624f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder, StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned training dataset\n",
    "train_file_path = '/Users/john/Downloads/leash-BELKA/cleaned_train.csv'\n",
    "df_train = dd.read_csv(train_file_path)\n",
    "\n",
    "# Define features and target\n",
    "X = df_train.drop('binds', axis=1)\n",
    "y = df_train['binds']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "categorizer = Categorizer()\n",
    "X = categorizer.fit_transform(X)\n",
    "encoder = DummyEncoder()\n",
    "X = encoder.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays for TensorFlow\n",
    "X_train = X_train.compute()\n",
    "X_test = X_test.compute()\n",
    "y_train = y_train.compute().values\n",
    "y_test = y_test.compute().values\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Load the test data for which we need to make predictions\n",
    "test_file_path = '/Users/john/Downloads/leash-BELKA/test.csv'  # Replace with the actual test data file path\n",
    "df_test = dd.read_csv(test_file_path)\n",
    "\n",
    "# Assuming the test data has an identifier column and the same features as the training data\n",
    "id_column = 'binds'  # Replace with the actual name of the identifier column\n",
    "X_submission = df_test.drop(id_column, axis=1)\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X_submission = categorizer.transform(X_submission)\n",
    "X_submission = encoder.transform(X_submission)\n",
    "\n",
    "# Ensure the same columns as in training\n",
    "X_submission = X_submission.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Standardize the test features\n",
    "X_submission = scaler.transform(X_submission)\n",
    "\n",
    "# Convert Dask arrays to NumPy arrays for TensorFlow\n",
    "X_submission = X_submission.compute()\n",
    "\n",
    "# Make predictions on the test data\n",
    "submission_predictions = model.predict(X_submission)\n",
    "submission_predictions = (submission_predictions > 0.5).astype(int)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    id_column: df_test[id_column].compute(),\n",
    "    'predictions': submission_predictions.flatten()\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_file_path = '/Users/john/Downloads/leash-BELKA/submission.csv'\n",
    "submission_df.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"Submission file created at: {submission_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b313db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
